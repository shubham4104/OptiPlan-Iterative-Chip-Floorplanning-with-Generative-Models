{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41609b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapePlacementEnv:\n",
    "    def __init__(self, grid_size, shapes):\n",
    "        \"\"\"\n",
    "        Initialize the environment.\n",
    "        :param grid_size: Tuple (n, m) representing the size of the grid.\n",
    "        :param shapes: List of tuples representing the shapes to be placed.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.shapes = shapes\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment for a new episode.\n",
    "        :return: The initial state of the environment.\n",
    "        \"\"\"\n",
    "        self.grid = np.zeros(self.grid_size)\n",
    "        self.current_shape_index = 0\n",
    "        return self.grid\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action in the environment.\n",
    "        :param action: Tuple (x, y) representing where to place the current shape.\n",
    "        :return: (next_state, reward, done)\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        shape = self.shapes[self.current_shape_index]\n",
    "        if self._can_place_shape(action, shape):\n",
    "            self._place_shape(action, shape)\n",
    "            reward = 1  # Example reward for successful placement\n",
    "        else:\n",
    "            reward = -1  # Example penalty for unsuccessful placement\n",
    "\n",
    "        self.current_shape_index += 1\n",
    "        if self.current_shape_index >= len(self.shapes):\n",
    "            done = True  # End of episode\n",
    "\n",
    "        return self.grid, reward, done\n",
    "\n",
    "    def _can_place_shape(self, position, shape):\n",
    "        \"\"\"\n",
    "        Check if a shape can be placed at the given position.\n",
    "        \"\"\"\n",
    "        x, y = position\n",
    "        shape_width, shape_height = shape\n",
    "        if x + shape_width > self.grid_size[0] or y + shape_height > self.grid_size[1]:\n",
    "            return False\n",
    "\n",
    "        # Check for overlap\n",
    "        for i in range(x, x + shape_width):\n",
    "            for j in range(y, y + shape_height):\n",
    "                if self.grid[i, j] == 1:\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _place_shape(self, position, shape):\n",
    "        \"\"\"\n",
    "        Place a shape on the grid at the given position.\n",
    "        \"\"\"\n",
    "        x, y = position\n",
    "        shape_width, shape_height = shape\n",
    "        for i in range(x, x + shape_width):\n",
    "            for j in range(y, y + shape_height):\n",
    "                self.grid[i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, n, m):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "\n",
    "        # Assuming input grid is n x m and single-channel\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Calculate the size of the grid after convolutional and pooling layers\n",
    "        conv_output_size = (n // 4) * (m // 4) * 64  # Assuming two 2x2 pooling layers\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(conv_output_size, 128)\n",
    "        # The output size will be dynamically determined based on the number of empty cells\n",
    "        self.fc2 = nn.Linear(128, n * m)  # Maximum possible actions\n",
    "\n",
    "    def forward(self, x, empty_cells_count):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Adjusting the output layer based on the number of empty cells\n",
    "        action_probs = self.fc2(x)[:, :empty_cells_count]  # Slicing to match empty cells count\n",
    "        return F.softmax(action_probs, dim=1)\n",
    "\n",
    "# Example usage\n",
    "n, m = 10, 10  # Grid size\n",
    "policy_net = PolicyNetwork(n, m)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)  # Example learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43632591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy_net):\n",
    "    state = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        #print(state)\n",
    "        #state = preprocess(state)  # Convert state to the correct input format\n",
    "        empty_cells_count = get_empty_cells_count(state)  # Count the number of empty cells\n",
    "        action_probs = policy_net(state, empty_cells_count)\n",
    "        \n",
    "        # Sample an action\n",
    "        action = select_action(action_probs)\n",
    "        log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "        \n",
    "        next_state, reward, done = env.step(action)  # Perform the action in the environment\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "    return log_probs, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b75f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(log_probs, returns):\n",
    "    policy_loss = []\n",
    "    for log_prob, R in zip(log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000  # Example number of episodes\n",
    "env = ShapePlacementEnv(5000,5000)\n",
    "for episode in range(num_episodes):\n",
    "    log_probs, rewards = run_episode(env, policy_net)\n",
    "    returns = compute_returns(rewards)\n",
    "    update_policy(log_probs, returns)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}: Average Reward: {sum(rewards) / len(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e69d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
